---
title: Earthquakes and the JMA
layout: post
---

There was a substantial earthquake (M7.6) in Noto Peninsula, Japan on January 1, 2024. [JMA](https://www.jma.go.jp/jma/index.html) used to have tables to view earthquake information and this seems to now be buried in a strange place in their UI. The map views are very useful, but it's not easy to get summary information.

It was trivial to find the source data they use. It's a JSON file `https://www.jma.go.jp/bosai/quake/data/list.json` and seems to contain only the information for the previous month.

There is another one that has information about long period ground movement: `https://www.jma.go.jp/bosai/ltpgm/data/list.json`

After loading the quake data into a pandas dataframe, it had a lot of extra stuff and some things that I had no idea what it meant. But after a bit more digging around I was able to figure out most of it and the parts that weren't necessary to keep.

First: imports and some clean up.

```python
import pandas as pd
import datetime
import matplotlib.pyplot as plt

# The json url
quakes_json_url = "https://www.jma.go.jp/bosai/quake/data/list.json"

# Load it into a dataframe
quakes = pd.read_json(quakes_json_url)

# Get rid of entries where the magnitude is unknown
mask = quakes.mag != "Ｍ不明"
quakes = quakes.loc[mask]

# Get rid of distant stuff (overseas)
mask = quakes.en_ttl != "Distant Earthquake Information"
quakes = quakes.loc[mask]

# Drop anything where ttl (Title) isn't "震源・震度情報" (Epicenter information)
# Some of the entries are more like "information" and I believe these are not necessary
mask = quakes.ttl == "震源・震度情報"
quakes = quakes.loc[mask]

# Extract the latitude, longitude and depth from 'cod' column
# We won't include the +/- for lat/long because Japan is all +
pattern = "[+-](\d{0,3}\.\d)[+-](\d{0,3}\.\d)[+-](\d*)/"
quakes[["latitude", "longitude", "depth"]] = quakes.cod.str.extract(pattern)

# Convert some columns to float/int
quakes.mag = pd.to_numeric(quakes.mag)
quakes.acd = quakes.acd.astype("int")
quakes.latitude = pd.to_numeric(quakes.latitude)
quakes.longitude = pd.to_numeric(quakes.longitude)
quakes.depth = (pd.to_numeric(quakes.depth) / 1000).astype("int") # convert to km first

# Convert 'at' to datetime but first will drop the offset
quakes["at"] = quakes["at"].apply(lambda x: x.replace("+09:00", "").replace("T", " "))
quakes["at"] = pd.to_datetime(quakes["at"])

# One other issue is that there are duplicates. I believe this is resolved by keeping the "ser" 2 
# There was one row, for example, that showed the big earthquake at M7.4, and another at M7.6
# But on the website the M7.6 is what is shown
# The eid is duplicated, and the first duplicate is the "ser" 1
quakes = quakes.drop_duplicates("eid")

# Drop columns we don't need
# ctt ?, rdt (Report Date Time), ttl (Title), en_ttl (English Title), ift ?, ser (Serial), cod, int, eid (Event ID)
# 'int' is intensity information for stations, but the json file is more detailed
# 'at' is utc datetime
quakes = quakes.drop(
    ["ttl", "en_ttl", "ctt", "rdt", "ift", "cod", "int", "ser"],
    axis=1
)

# Set index to eid
quakes = quakes.set_index("eid")

# Rename columns
quakes = quakes.rename(
    columns={
        "anm": "area_name", 
        "en_anm": "area_name_en", 
        "maxi": "max_intensity", 
        "at": "date_time",
        "acd": "area_code",
    })


quakes.head()
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date_time</th>
      <th>area_name</th>
      <th>area_code</th>
      <th>mag</th>
      <th>max_intensity</th>
      <th>json</th>
      <th>area_name_en</th>
      <th>latitude</th>
      <th>longitude</th>
      <th>depth</th>
    </tr>
    <tr>
      <th>eid</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>20240102182939</th>
      <td>2024-01-02 18:29:00</td>
      <td>石川県能登地方</td>
      <td>390</td>
      <td>4.3</td>
      <td>3</td>
      <td>20240102183318_20240102182939_VXSE5k_1.json</td>
      <td>Noto, Ishikawa Prefecture</td>
      <td>37.5</td>
      <td>137.3</td>
      <td>10</td>
    </tr>
    <tr>
      <th>20240102182547</th>
      <td>2024-01-02 18:25:00</td>
      <td>石川県能登地方</td>
      <td>390</td>
      <td>3.2</td>
      <td>1</td>
      <td>20240102182918_20240102182547_VXSE5k_1.json</td>
      <td>Noto, Ishikawa Prefecture</td>
      <td>37.2</td>
      <td>136.7</td>
      <td>10</td>
    </tr>
    <tr>
      <th>20240102182034</th>
      <td>2024-01-02 18:20:00</td>
      <td>石垣島近海</td>
      <td>854</td>
      <td>4.4</td>
      <td>1</td>
      <td>20240102182339_20240102182034_VXSE5k_1.json</td>
      <td>Adjacent Sea of​ Ishigakijima Island</td>
      <td>24.8</td>
      <td>124.3</td>
      <td>70</td>
    </tr>
    <tr>
      <th>20240102180754</th>
      <td>2024-01-02 18:07:00</td>
      <td>石川県能登地方</td>
      <td>390</td>
      <td>2.9</td>
      <td>2</td>
      <td>20240102181039_20240102180754_VXSE5k_1.json</td>
      <td>Noto, Ishikawa Prefecture</td>
      <td>37.2</td>
      <td>136.7</td>
      <td>10</td>
    </tr>
    <tr>
      <th>20240102180701</th>
      <td>2024-01-02 18:07:00</td>
      <td>和歌山県北部</td>
      <td>550</td>
      <td>3.1</td>
      <td>1</td>
      <td>20240102181014_20240102180701_VXSE5k_1.json</td>
      <td>Northern Wakayama Prefecture</td>
      <td>33.9</td>
      <td>135.1</td>
      <td>10</td>
    </tr>
  </tbody>
</table>
</div>


This left us with a dataframe that has a number of nice columns:

```
['date_time', 'area_name', 'area_code', 'mag', 'max_intensity', 'json',
 'area_name_en', 'latitude', 'longitude', 'depth']
```

The code for Ishikawa-ken is `390` - it's fairly easy to determine that by looking at the data. There have been around 24 hours that have passed since the M7.6 earthquake. How many aftershocks have been in the area?


```python
# number of quakes in 390 in a 24 hour time frame
start = datetime.datetime.fromisoformat("2024-01-01 16:00") # 4pm, new years
end = datetime.datetime.fromisoformat("2024-01-02 16:00") # 24hrs later

dt_mask = (quakes.date_time > start) & (quakes.date_time < end)
location_mask = quakes.area_code == 390
quakes[location_mask & dt_mask].shape[0]
```

```
116
```

116 quakes in 24 hours. That's a lot of quakes... one every 12 minutes or so. Well, really there were 115 because there was an M5+ at around 4:06pm.

### Plotting the magnitude

```python
ishikawa_quakes = quakes[location_mask & dt_mask]

# sort in reverse, and reset the index
ishikawa_quakes = ishikawa_quakes.sort_values("date_time", ascending=True).reset_index().drop("eid", axis=1)

# plot it
ishikawa_quakes.mag.plot(
    title="Quake Magnitude, Ishikawa", 
    xlabel="Date/Time",
    ylabel="Magnitude",
    figsize=(30,10),
    kind="bar",
)

```

![png](/assets/2024_01_02_ishikawa_quake_magnitude.png)


It's important to consider the depth of the earthquakes as well, but I was mostly curious to just plot the magnitude and see how many quakes there were.

### More details...

The `json` column is interesting. Each json file contains a complete listing of each intensity station affected by the listed earthquake. It also has the names for prefectures, areas, cities, and the station. The intensity for each station is recorded. If you really wanted to dig in, you could create another dataframe with this flattened data, do a merge, and then you could see what earthquakes affected a station near you.
